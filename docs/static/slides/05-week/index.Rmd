---
title: "Resampling Methods"
subtitle: "AU STAT-427/627"
author: "Emil Hvitfeldt"
date: "2021-2-16"
output:
  xaringan::moon_reader:
    css: ["theme.css", "default"]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
---

```{r include=FALSE}
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines) == 1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

opts_chunk$set(
  echo = TRUE,
  fig.width = 7, 
  dpi = 300,
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px")

library(tidymodels)
```

```{r, echo = FALSE}
library(sass)
sass(sass_file("theme.sass"), output = "theme.css")
```

<div style = "position:fixed; visibility: hidden">
$$\require{color}\definecolor{orange}{rgb}{1, 0.603921568627451, 0.301960784313725}$$
$$\require{color}\definecolor{blue}{rgb}{0.301960784313725, 0.580392156862745, 1}$$
$$\require{color}\definecolor{pink}{rgb}{0.976470588235294, 0.301960784313725, 1}$$
</div>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      orange: ["{\\color{orange}{#1}}", 1],
      blue: ["{\\color{blue}{#1}}", 1],
      pink: ["{\\color{pink}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>

<style>
.orange {color: #FF9A4D;}
.blue {color: #4D94FF;}
.pink {color: #F94DFF;}
</style>


```{r flair_color, echo=FALSE}
library(flair)
orange <- "#FF9A4D"
blue <- "#4D94FF"
pink <- "#F94DFF"
```

# Motivation

We are already familiar with train-test splits

The main downside to to train-test splits so far is that we can only use them once

This means we effectively can't make any decisions about the models we are using

---

# Resampling

Resampling estimates of performance can generalize to new data


---

# Resampling Workflow

.center[
![:scale 70%](images/resampling.svg)
]

---

# Resampling Workflow

The resampling is only conducted on the training set

We are still keeping the test set.The test set is not involved. 

For each iteration of resampling, the data are partitioned into two subsamples:

- The model is fit with the .orange[analysis set]
- The model is evaluated with the .blue[assessment set]

---

# Resampling Workflow

We have effectively created many train-test split out of our training data set.

The .blue[challange] here now becomes how we are creating these resample sets

---

# Resampling Workflow

Suppose we generate 10 different resamples

This mean that we will be:

- Fitting 10 different models
- Perform predictions 10 times
- Prodice 10 sets of performance statistics

The final estimate of the .blue[performance] of the model will be the average of these 10 models

---

# Resampling Workflow

If the resampling is done in a appropiate way then this average has very good generalization properties

---

# Leave-One-Out Cross Validation

- 1 observation is used as the .blue[assessment set]
- The remaining observations make up the .orange[analysis set]

Notes:

We are fitting the model on $n-1$ observations and a prediction $\hat y_1$ is made on the .blue[assessment set] using the value $x_1$

---

# Leave-One-Out Cross Validation

Since $(x_1, y_1)$ is not used in the fitting process, then $MSE_1 = (y_1 - \hat y_1)^2$ provides an approximately unbiased estimate for the test error.

While this estimate is approximately unbiased, it is quite poor since it is highly variable

---

# Leave-One-Out Cross Validation

We can repeat this for 

- $MSE_2 = (y_2 - \hat y_2)^2$
- $MSE_3 = (y_3 - \hat y_3)^2$
- ...
- $MSE_n = (y_n - \hat y_n)^2$

 to get $n$ estimates of the test error
 
---

# Leave-One-Out Cross Validation

The LOOCV estimate of the test MSE is

$$CV_{(n)} = \dfrac{1}{n} \sum^n_{i=1}MSE_i$$

---

# Leave-One-Out Cross Validation

## Pros

The LOOCV estimate of the test MSE is going to have low bias

There is no randomness in the LOOCV estimate

## Cons

You need a lot of computational power even for modest data sets

(Some models don't need to be repeatedly refit)

---

# K-Fold Cross Validation

Could we think of a compromise between fitting 1 model and $n$ models?

.pink[K-Fold Cross Validation] has an answer:

Randomly divide the observations into $k$ groups (or .blue[folds]) or approximately equal size

---

# K-Fold Cross Validation

Randomly divide the observations into $k$ groups (or .blue[folds]) or approximately equal size

- 1 .blue[fold] is used as the .blue[assessment set]
- The remaining .blue[folds] make up the .orange[analysis set]

Everything else happens as before.

We now get fewer performance metrics, BUT they are each less variable

---

```{css, echo=FALSE}
.footnote {
    position: absolute;
    bottom: 0em;
    padding-right: 4em;
    font-size: 90%;
}
```

---
background-image: url(images/cross-validation/Slide2.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---
background-image: url(images/cross-validation/Slide3.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---
background-image: url(images/cross-validation/Slide4.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---
background-image: url(images/cross-validation/Slide5.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---
background-image: url(images/cross-validation/Slide6.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---
background-image: url(images/cross-validation/Slide7.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---
background-image: url(images/cross-validation/Slide8.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---
background-image: url(images/cross-validation/Slide9.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---
background-image: url(images/cross-validation/Slide10.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---
background-image: url(images/cross-validation/Slide11.png)
background-size: contain

.footnote[
Art by [Alison Hill](https://alison.rbind.io/)
]

---

# Cross validation

When we perform cross-validation our goal might be to determine how well a given model is expected to perform on new data

Other times we are using cross-validation to find the best model/hyperparameters

---

# Bias-Variance radeoff of LOOCV and k-fold Cross Validation

LOOCV has a lower bias then k-fold CV

However since the mean of many highly correlated quantities has higher variance then the mean of many not correlated quantities, we have that LOOCV has a higher variance then k-fold CV

---

# Rsample

We are back with `rsample`

```{r}
library(rsample)

mtcars
```

---

# Rsample

.pull-left[
We can use the `vfold_cv()` function on a data.frame to create a `vfold_cv` object

```{r, eval=FALSE}
mtcars_folds <- vfold_cv(mtcars, v = 4)
mtcars_folds
```
]

.pull-right[
```{r, echo=FALSE}
mtcars_folds <- vfold_cv(mtcars, v = 4)
mtcars_folds
```
]


---

# Rsample

An under the hood, we have 4 analysis/assesment splits similar to `initial_split()`

.pull-left[
```{r, eval=FALSE}
mtcars_folds <- vfold_cv(mtcars, v = 4)
mtcars_folds$splits
```
]

.pull-right[
```{r, echo=FALSE}
mtcars_folds <- vfold_cv(mtcars, v = 4)
mtcars_folds$splits
```
]

---

# Using resamples in action

We start by creating a linear regression sepecification

```{r}
library(parsnip)
linear_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")
```

---

# Workflows

Simple package that helps us formulate more about what happens to our model.

Main functions are `workflow()`, `add_model()`, `add_formula()` or `add_variables()` (we will see `add_recipe()` later in the course)

```{r}
library(workflows)

linear_wf <- workflow() %>%
  add_model(linear_spec) %>%
  add_formula(mpg ~ disp + hp + wt)
```

---

# Workflows

This allows up to combine the model with what variables it should expect

.pull-left[
```{r, eval=FALSE}
library(workflows)

linear_wf <- workflow() %>%
  add_model(linear_spec) %>%
  add_formula(mpg ~ disp + hp + wt)
linear_wf
```
]

.pull-right[
```{r, echo=FALSE}
library(workflows)

linear_wf <- workflow() %>%
  add_model(linear_spec) %>%
  add_formula(mpg ~ disp + hp + wt)
linear_wf
```
]

---

`add_variables()` allows for a different way of specifying the the response and predictors in our model

# Workflows

.pull-left[
```{r, eval=FALSE}
library(workflows)

linear_wf <- workflow() %>%
  add_model(linear_spec) %>%
  add_variables(outcomes = mpg,
                predictors = c(disp, hp, wt))
linear_wf
```
]

.pull-right[
```{r, echo=FALSE}
library(workflows)

linear_wf <- workflow() %>%
  add_model(linear_spec) %>%
  add_variables(outcomes = mpg,
                predictors = c(disp, hp, wt))
linear_wf
```
]

---

# Workflows

You can use a `workflow` just like a parsnip object and fit it directly

```{r}
fit(linear_wf, data = mtcars)
```

---

# Tune

We introduce the **tune** package. This package helps us fit many models in a controlled manner in the tidymodels framework. It relies heavily on parsnip and rsample

---

# Tune

We can use `fit_resamples()` to fit the workflow we created within each resample

```{r}
library(tune)

linear_fold_fits <- fit_resamples(
    linear_wf,
    resamples = mtcars_folds
)
```

---

# Tune

The results of this resampling comes as a data.frame

```{r}
linear_fold_fits
```

---

# Tune

`collect_metrics()` can be used to extract the CV estimate

```{r}
library(tune)

linear_fold_fits <- fit_resamples(
    linear_wf,
    resamples = mtcars_folds
)

collect_metrics(linear_fold_fits)
```

---

# Tune

Setting `summarize = FALSE` in `collect_metrics()` Allows us the see the individual performance metrics for each fold

```{r}
collect_metrics(linear_fold_fits, summarize = FALSE)
```

---


.pull-left[
# Tune

There are some settings we can set with `control_resamples()`.

One of the most handy ones (IMO) is `verbose = TRUE`

```{r, eval=FALSE}
library(tune)

linear_fold_fits <- fit_resamples(
  linear_wf,
  resamples = mtcars_folds,
  control = control_resamples(verbose = TRUE)
)
```
]

.pull-right[
.center[
![:scale 90%](images/verbose.png)
]
]

---

# Tune

We can also directly specify the metrics that are calculated within each resample

```{r}
library(tune)

linear_fold_fits <- fit_resamples(
    linear_wf,
    resamples = mtcars_folds, 
    metrics = metric_set(rmse, rsq, mase)
)

collect_metrics(linear_fold_fits)
```
