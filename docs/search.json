{
  "articles": [
    {
      "path": "01-labs.html",
      "title": "Lab 01 - Hello R!",
      "author": [],
      "contents": "\nLearning goals\nGet acquainted with R and RStudio, which we will be using throughout the course to analyze data as well as to learn the statistical concepts discussed in the course.\nAppreciate the value of visualization in exploring the relationship between variables.\nStart using R for building plots and calculating summary statistics.\nTerminology\nWe’ve already thrown around a few new terms, so let’s define them before we proceed.\nR: Name of the programming language we will be using throughout the course.\nRStudio: An integrated development environment for R. In other words, a convenient interface for writing and running R code.\nI like to think of R as the engine of the car, and RStudio is the dashboard.\nStarting slow\nAs the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\nAnd to make versioning simpler, this is a solo lab. Additionally, we want to make sure everyone gets a significant amount of time at the steering wheel.\nGetting started\nDownload R\nIf you don’t have R installed.\nGo to the CRAN and download R, make sure you get the version that matches your operating system.\nIf you have R installed\nIf you have R installed run the following code\n\n\nR.version\n\n\n               _                           \nplatform       x86_64-apple-darwin17.0     \narch           x86_64                      \nos             darwin17.0                  \nsystem         x86_64, darwin17.0          \nstatus                                     \nmajor          4                           \nminor          0.2                         \nyear           2020                        \nmonth          06                          \nday            22                          \nsvn rev        78730                       \nlanguage       R                           \nversion.string R version 4.0.2 (2020-06-22)\nnickname       Taking Off Again            \n\nThis should tell you what version of R you are currently using. If your R version is lower then 3.6.0 I would strongly recommend updating. In general it is a good idea to update your R version, unless you have a project right now that depend on a specific version of R.\nDownload RStudio\nWe recommend using RStudio as your IDE if you don’t already have it installed. You can go to the RStudio website to download and install the software.\nLaunch RStudio\nYou can also open the RStudio application first and then create a project by going file -> new project...\nCreate a new Rmarkdown file\nfile -> new file -> R markdown...\nHello RStudio!\nRStudio is comprised of four panes.\nOn the bottom left is the Console, this is where you can write code that will be evaluated. Try typing 2 + 2 here and hit enter, what do you get?\nOn the bottom right is the Files pane, as well as other panes that will come handy as we start our analysis.\nIf you click on a file, it will open in the editor, on the top left pane.\nFinally, the top right pane shows your Environment. If you define a variable it would show up there. Try typing x <- 2 in the Console and hit enter, what do you get in the Environment pane?\nPackages\nR is an open-source language, and developers contribute functionality to R via packages. In this lab we will work with three packages: palmerpenguins which contains the dataset, and tidyverse which is a collection of packages for doing data analysis in a “tidy” way.\nLoad these packages by running the following in the Console.\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\n\n\nIf you haven’t installed these packages yet and R complains, then you can install these packages by running the following command. (Note that R package names are case-sensitive)\n\n\ninstall.packages(c(\"tidyverse\", \"palmerpenguins\"))\n\n\n\nNote that the packages are also loaded with the same commands in your R Markdown document.\nWarm up\nBefore we introduce the data, let’s warm up with some simple exercises.\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\nYAML\nOpen the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document.\nData\nThe data frame we will be working with today is called penguins and it’s in the palmerpenguins package.\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\npenguins\n\n\n\ncount the number of species and islands with dplyr::count()\nVisualize the distribution of body_mass_g with ggplot\n\n\nggplot(penguins, aes(body_mass_g)) +\n  geom_histogram()\n\n\n\nLook at the correlation between body_mass_g and some of the other variables\n\nggplot(penguins, aes(body_mass_g, ___)) +\n  geom_point()\n\nModeling\nFit a linear model using parsnip to model body_mass_g\n\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\nlm_fit <- lm_spec %>%\n  fit(___ ~ species + island + bill_length_mm + bill_depth_mm + flipper_length_mm, \n      data = penguins)\n\nlm_fit\n\nGet parameter estimates:\n\n\ntidy(lm_fit)\n\n\n\nFigures\nYou’re done with the data analysis exercises, but we’d like you to do two more things:\n\nResize your figures:\nClick on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the pop up dialogue box go to the Figures tab and change the height and width of the figures, and hit OK when done. Then, knit your document and see how you like the new sizes. Change and knit again and again until you’re happy with the figure sizes. Note that these values get saved in the YAML.\n\nYou can also use different figure sizes for different figures. To do so click on the gear icon within the chunk where you want to make a change. Changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your R Markdown document as well.\n\nChange the look of your report:\nOnce again click on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the General tab of the pop up dialogue box try out different syntax highlighting and theme options. Hit OK and knit your document to see how it looks. Play around with these until you’re happy with the look.\nOptional\nIf you have time you can explore the different ways you can add styling to your rmarkdown document.\nHere is a cheatsheet\nand a markdown cheatsheet\nThis set of lab exersixes have been adopted from Mine Çetinkaya-Rundel’s class Introduction to Data Science.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:33-07:00"
    },
    {
      "path": "01-readings.html",
      "title": "Readings - week 1",
      "author": [],
      "contents": "\nRead chapter 2 of “An Introduction to Statistical Learning”. This is a big picture chapter that lays the foundation of the rest of the book. It is not expected to have read this before class.\nChapter 1 serves as an introduction to the book, data, and notation. Can be read or skimmed through if you want.\nWe will be using R which can be downloaded here. Additionally, it is also advised to use RStudio which can be downloaded here, but any IDE will work.\nWe will be using the tidymodels ecosystem of packages designed for modeling. If you haven’t already you should install these packages along with tidyverse.\n\n\n\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download\n\n\n\n",
      "last_modified": "2021-03-21T01:41:33-07:00"
    },
    {
      "path": "assignments-01.html",
      "title": "Assignment 1",
      "author": [],
      "contents": "\nExercise 1 (7 points)\nFor each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\nThe sample size \\(n\\) is extremely large, and the number of predictors \\(p\\) is small.\nThe number of predictors \\(p\\) is extremely large, and the number of observations \\(n\\) is small.\nThe relationship between the predictors and response is highly non-linear.\nThe variance of the error terms, is extremely high.\nExercise 2 (6 points)\nDescribe the difference between a parametric and non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a noon-parametric approach)? What are its disadvantages?\nExercise 3 (6 points)\nCarefully explain the the difference between the KNN classifier and KNN regression methods. Name a downside when using this model on very large data.\nExercise 4 (7 points)\nSuppose we have a data set with five predictors, \\(X1 =\\) GPA, \\(X2 =\\) extracurricular activities (EA), \\(X3 =\\) Gender (1 for Female and 0 for Male), \\(X4 =\\) Interaction between GPA and EA, and \\(X5 =\\) Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(\\beta_0 = 50\\), \\(\\beta_1 = 20\\), \\(\\beta_2 = 0.07\\), \\(\\beta_3 = 35\\), \\(\\beta_4 = 0.01\\), \\(\\beta_5 = - 10\\).\nWhich answer is correct, and why?\nFor a fixed value of EA and GPA, males earn more on average than females.\nFor a fixed value of EA and GPA, females earn more on average than males.\nFor a fixed value of EA and GPA, males earn more on average than females provided that the GPA is high enough.\nFor a fixed value of EA and GPA, females earn more on average than males provided that the GPA is high enough.\n\nPredict the salary of a female with EA of 110 and a GPA of 4.0.\nTrue or false: Since the coefficient for the GPA/EA interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\nExercise 5 (9 points)\nThis question should be answered using the biomass data set.\n\n\nlibrary(tidymodels)\ndata(\"biomass\")\n\n\n\nFit a multiple regression model to predict HHV using carbon, hydrogen and oxygen.\nProvide an interpretation of each coefficient in the model. Be careful, note the values Cruise is able to take.\nWrite out the model in equation form.\nFor which the predictors can you reject the null hypothesis \\(H_0: \\beta_j = 0\\)?\nOn the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.\nHow well do the models in (a) and (e) fit the data? How big was the effect of removing the predictor?\n\n\n\n",
      "last_modified": "2021-03-21T01:41:34-07:00"
    },
    {
      "path": "assignments-02.html",
      "title": "Assignment 2",
      "author": [],
      "contents": "\nExercise 1 (10 points)\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1\\) = hours studied, \\(X_2\\) = undergrad GPA, and \\(Y\\) = receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta}_0=-6\\), \\(\\hat{\\beta}_1=0.05\\), \\(\\hat{\\beta}_2=1\\).\nEstimate the probability that a student who studies for 40 hours and has an undergrad GPA of \\(3.5\\) gets an A in the class.\nHow many hours would that student in part (a) need to study to have a 50% chance of getting an A in the class?\nExercise 2 (10 points)\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First, we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next, we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\nExercise 3 (15 points)\nIn this exercise, we will explore a data set about cars called auto which you can find here.\nThe data set contains 1 factor variable and 6 numeric variables. The factor variable mpg has two levels high and low indicating whether the car has a high or low miles per gallon. We will in this exercise investigate if we are able to use a logistic regression classifier to predict if a car has high or low mpg from the other variables.\nRead in the data and create a test-train rsplit object of auto using initial_split(). Use default arguments for initial_split().\nCreate the training and testing data set with training() and testing() respectively.\nFit a logistic regression model using logistic_reg(). Use all the 6 numeric variables as predictors (a formula shorthand is to write mpg ~ . where . means everything. Remember to fit the model only using the training data set.\nInspect the model with summary() and tidy(). Which of the variables are significant?\nPredict values for the training data set and save them as training_pred.\nUse the following code to calculate the training accuracy\n\n\nbind_cols(\n  training_pred,\n  auto_training\n) %>%\n  accuracy(truth = mpg, estimate = .pred_class)\n\n\n\n(auto_training should be renamed to match your training data set if needed.)\nPredict values for the testing data set and use the above code to calculate the testing accuracy. Compare.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:34-07:00"
    },
    {
      "path": "assignments-03.html",
      "title": "Assignment 3",
      "author": [],
      "contents": "\nExercise 1 (10 points)\nYou will in this exercise examine the differences between LDA and QDA.\nIf the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\nIf the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\nIn general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy or QDA relative to LDA to improve, decline, or be unchanged? Why?\nTrue or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\nExercise 2 (10 points)\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First, we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next, we use 1-nearest neighbors (i.e. K = 1) and get an average error rate (averaged over both test and training data set) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\nExecise 3 (15 points)\nThis exercise should be answered using the Weekly data set, which is part of the LSLR package. If you don’t have it installed already you can install it with\n\n\ninstall.packages(\"ISLR\")\n\n\n\nTo load the data set run the following code\n\n\nlibrary(ISLR)\ndata(\"Weekly\")\n\n\n\nThis data is similar in nature to the Smarket data from chapter 4’s lab, it contains 1089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\nProduce some numerical and graphical summaries of the data. Does there appear to be any patterns?\nUse the whole data set to perform a logistic regression (with logistic_reg()) with Direction as the response and the five lag variables plus Volume as predictors. Use the summary() (remember to do summary(model_fit$fit)) function to print the results. Do any of the predictors appear to be statistically significant? if so, which ones?\nUse conf_int() and accuracy() from yardstick package to calculate the confusion matrix and the accuracy (overall fraction of correct predictions). Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\nSplit the data into a training and testing data set using the following code\n\n\nweekly_training <- Weekly %>% filter(Year <= 2008)\n weekly_testing <- Weekly %>% filter(Year > 2008)\n \n\n\nNow fit the logistic regression model using the training data, with Lag2 as the only predictor. Compute the confusion matrix and accuracy metric using the testing data set.\nRepeat (e) using LDA.\nRepeat (e) using QDA.\nRepeat (e) using KNN with K = 1.\nWhich of these methods appear to provide the best results on the data?\n(Optional) Experiment with different combinations of predictors for each of the methods. Report the variables, methods, and associated confusion matrix that appears to provide the best results on the held-out data. Note that you can also experiment with different values of K in KNN. (This kind of running many many models and testing on the testing data set many times is not good practice. We will look at ways in later weeks on how we can properly explore multiple models.)\n\n\n\n",
      "last_modified": "2021-03-21T01:41:35-07:00"
    },
    {
      "path": "assignments-04.html",
      "title": "Assignment 4",
      "author": [],
      "contents": "\nExercise 1 (10 points)\nReview of k-fold cross-validation.\nExplain how k-fold cross-validation is implemented.\nWhat are the advantages and disadvantages of k-fold cross-validation relative to\nThe validation set approach\nLOOCV\n\nExercise 2 (10 points)\nDenote whether the following statements are true or false. Explain your reasoning.\nWhen \\(k = n\\) the cross-validation estimator is approximately unbiased for the true prediction error.\nWhen \\(k = n\\) the cross-validation estimator will always have a low variance.\nStatistical transformations on the predictors, such as scaling and centering, must be done inside each fold.\nExercise 3 (15 points )\nThis exercise should be answered using the Weekly data set, which is part of the LSLR package. If you don’t have it installed already you can install it with\n\n\ninstall.packages(\"ISLR\")\n\n\n\nTo load the data set run the following code\n\n\nlibrary(ISLR)\ndata(\"Weekly\")\n\n\n\nCreate a test and training set using initial_split(). Split proportion is up to you. Remember to set a seed!\nCreate a logistic regression specification using logistic_reg(). Set the engine to glm.\nCreate a 5-fold cross-validation object using the training data, and fit the resampled folds with fit_resamples() and Direction as the response and the five lag variables plus Volume as predictors. Remember to set a seed before creating k-fold object.\nCollect the performance metrics using collect_metrics(). Interpret.\nFit the model on the whole training data set. Calculate the accuracy on the test set. How does this result compare to results in d. Interpret.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:36-07:00"
    },
    {
      "path": "assignments-05.html",
      "title": "Assignment 5",
      "author": [],
      "contents": "\nExercise 1 (15 points)\nWe will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of \\(n\\) observations.\nWhat is the probability that the first bootstrap observation is not the \\(j\\)th observation from the original sample? Justify your answer.\nWhat is the probability that the second bootstrap observation is not the \\(j\\)th observation from the original sample? Justify your answer.\nArgue that the probability that the \\(j\\)th observation is not in the bootstrap sample is \\((1-1/n)^n\\).\nWhen \\(n = 5\\) what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nWhen \\(n = 100\\) what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nWhen \\(n = 10,000\\) what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nCreate a plot that displays, for each integer value of \\(n\\) from 1 to 100,000 the probability that the \\(j\\)th observation is in the bootstrap sample. Comment on what you observe.\nWe will now investigate numerically that a bootstrap sample of size \\(n = 100\\) contains the \\(j\\)th observation. Here \\(j = 4\\). We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.\n\n\nset.seed(  ) # set a seed here\n\nstore <- integer(10000)\n\nfor (i in seq_along(store)) {\n  store[i] <- sum(sample(seq_len(100), replace = TRUE) == 4) > 0\n}\n\nmean(store)\n\n\n\nComment on the results obtained.\nExercise 2 (10 points)\nSuppose that we use some statistical learning method to make a prediction for the response \\(Y\\) for a particular value of the predictor \\(X\\).\nCarefully describe how we might estimate the standard deviation of our prediction.\nIs this procedure depends on what statistical learning method we are using?\nExercise 3 (10 points)\nThis exercise should be answered using the Default data set, which is part of the LSLR package. If you don’t have it installed already you can install it with\n\n\ninstall.packages(\"ISLR\")\n\n\n\nTo load the data set run the following code\n\n\nlibrary(ISLR)\ndata(\"Default\")\n\n\n\nUse the parsnip package to fit a logistic regression on the default data set. default is the response and income and balance are the predictors. Then use summary() on the fitted model to determine the estimated standard errors for the coefficients associated with income and balance. Comment on the estimated standard errors.\nUse the bootstraps() function from the rsample package to generate 25 bootstraps of Default.\nRun the following code. Change boots to the name of the bootstrapping object created in the previous question. This will take a minute or two to run. Comment on\n\n\n# This function takes a `bootstrapped` split object, and fits a logistic model\nfit_lr_on_bootstrap <- function(split) {\n    logistic_reg() %>%\n    set_engine(\"glm\") %>%\n    set_mode(\"classification\") %>%\n    fit(default ~ income + balance, analysis(split))\n}\n\n# This code uses `map()` to run the model inside each split. Then it used\n# `tidy()` to extract the model estimates the parameter\nboot_models <-\n  boots %>% \n  mutate(model = map(splits, fit_lr_on_bootstrap),\n         coef_info = map(model, tidy))\n\n# This code extract the estimates for each model that was fit\nboot_coefs <-\n  boot_models %>% \n  unnest(coef_info)\n\n# This code calculates the standard deviation of the estimate\nsd_estimates <- boot_coefs %>%\n  group_by(term) %>%\n  summarise(std.error_est = sd(estimate))\nsd_estimates\n\n\n\nComment on the estimated standard errors obtained using the summary() function on the first model and the estimated standard errors you found using the above code.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:37-07:00"
    },
    {
      "path": "assignments-06.html",
      "title": "Assignment 6",
      "author": [],
      "contents": "\nExercise 1\nExplain the assumptions we are making when performing Principle Component Analysis (PCA). What happens when these assumptions are violated?\nExercise 2\nAnswer the following questions regarding Principle Component Analysis.\nIs it important to standardize before applying PCA?\nShould one remove highly correlated variables before doing PCA?\nWhat will happen when eigenvalues are roughly equal?\nCan PCA be used to reduce the dimensionality of a highly nonlinear data set?\nExercise 3\nYou will in this exercise explore a data set using PCA. The data comes from the #tidytuesday project and is about Student Loan Payments.\nLoad in the data using the following script.\n\n\nlibrary(tidymodels)\nloans <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-26/loans.csv\") %>%\n  select(-agency_name, -added) %>%\n  drop_na()\n\n\n\nUse the prcomp() function to perform PCA on the loans data set. Set scale. = TRUE to perform scaling. What results are contained in this object? (hint: use the names() function)\nCalculate the amount of variance explained by each principal component. (hint: look at ?broom:::tidy.prcomp)\nUse the tidy() function to extract the the loadings. Which variable contributed most to the first principle component? Second Component?\nUse the augment() function to get back the transformation and create a scatter plot of any two components of your choice.\nExercise 4\nIn this exercise, you are tasked to predict the weight of an animal in a zoo, based on which words are used to describe it. The animals data set can be downloaded here.\nThis data set contains 1001 variables. The first variable weight is the natural log of the mean weight of the animal. The remaining variables are named tf_* which shows how many times the word * appears in the description of the animal.\nUse {tidymodels} to set up a workflow to train a PC regression. We can do this by specifying a linear regression model, and create a preprocessor recipe with {recipes} that applies PCA transformation on the predictors using step_pca(). Use the threshold argument in step_pca() to only keep the principal components that explain 90% of the variance.\nHow well does this model perform on the testing data set?\n\n\n\n",
      "last_modified": "2021-03-21T01:41:38-07:00"
    },
    {
      "path": "assignments-07.html",
      "title": "Placeholder",
      "author": [],
      "contents": "\nExercise 1 (12 points)\nFor part (a) through (c) indicate which of the statements are correct. Justify your answers.\nThe lasso, relative to least squares, is:\nMore flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\nMore flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\nLess flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\nLess flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n\nRepeat (a) for ridge regression relative to least squares.\nRepeat (a) for non-linear methods relative to least squares.\nExercise 2 (10 points)\nSuppose we estimate the regression coefficients in a linear regression model by minimizing\n\\[\n\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum^p_{j=1}\\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\]\nfor a particular value of \\(\\lambda\\). For part (a) through (c) indicate which of the statements are correct. Justify your answers.\nAs we increase \\(\\lambda\\) from 0, the training RSS will:\nIncrease initially, and then eventually start decreasing in an inverted U shape.\nDecrease initially, and then eventually start increasing in a U shape.\nSteadily increase.\nSteadily decrease.\nRemain constant.\n\nRepeat (a) for test RSS.\nRepeat (a) for variance.\nRepeat (a) for squared bias.\nRepeat (a) for the irreducible error.\nExercise 3 (13 points)\nIn this exercise, you are tasked to predict the weight of an animal in a zoo, based on which words are used to describe it. The animals data set can be downloaded here.\nThis data set contains 1001 variables. The first variable weight is the natural log of the mean weight of the animal. The remaining variables are named tf_* which shows how many times the word * appears in the description of the animal.\nFit a lasso regression model to predict weight based on all the other variables.\nUse the tune package to perform hyperparameter tuning to select the best value of \\(\\lambda\\). Use 10 bootstraps as the resamples data set.\nHow well does this model perform on the testing data set?\n\n\n\n",
      "last_modified": "2021-03-21T01:41:39-07:00"
    },
    {
      "path": "assignments-Midterm.html",
      "title": "Midterm",
      "author": [],
      "contents": "\nExercise 1 (15 points)\nExplain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction.\nWe collect several different measurements of salmon swimming through a small river. These measurements include; length, width, weight, color, and time of day the measurement was taken. Later down the river data is collected whether the fish survived the journey.\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 40 similar products that were previously launched. For each product, we have recorded whether it was a big success or failure, price charged for the product, marketing budget, competition price, and ten other variables. This product is scheduled to be rolled out to multiple stores across the midwest.\nYou are a real estate agent and you are looking at predicting the sale price of your upcoming houses based on lot size, house size, number of bathrooms, number of bedrooms, and presence of a pool.\nExercise 2 (15 points)\nWhat are the advantages and disadvantages of a very flexible approach compared to a less flexible approach for regression or classification? If you were you draw the decision boundary for a very flexible classification model how would it look? Under what circumstances might a more flexible approach be preferred to a less flexible approach?\nExercise 3 (15 points)\nExplain the differences between K-nearest neighbor and linear regression for a general regression task. Under what circumstances would a K-nearest neighbor approach perform better than a linear model. The performance here is measured using an appropriate performance metric calculated on the training data set.\nExercise 4 (15 points)\nExplain how the scaling of predictor variables will or won’t be affecting the model fit for K-nearest neighbors, logistic regression, LDA, and QDA.\nExercise 5 (15 points)\nSuppose you are given a data set and told to perform a clustering analysis to determine how many clusters are present. Explain how you would go about doing that.\nExercise 6 (15 points)\nSuppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.\nExercise 7 (60 points)\nIn this exercise, you will try to fit a classification model. You are given a data set with a response and 10 numeric predictors. You are to fit 2 knn models one with (K = 1) and one with (K = 2), 1 LDA, and one QDA. The data have already been split for you and can be downloaded here vowel_train and vowel_test. Use K-fold cross-validation with K = 10 with accuracy as the performance metric to select 1 of the 4 models. Fit this one model on the training data set, predict on the testing data set, and calculate the testing accuracy and construct a confusion matrix. Comment on your results.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:39-07:00"
    },
    {
      "path": "assignments.html",
      "title": "Assignments",
      "author": [],
      "contents": "\nWe will 10 weekly assignments, 1 midterm, and a final project in this class. The placement of these is located in the schedule.\nAssignments are to be turned in on Blackboard. Assignments will be available no later than 3 days before class and are due to be turned in on the following Sunday. Specific times can be found on Blackboard. The assignment will contain a mix of conceptual, technical, and coding exercises.\nThe midterm and final project will follow a different schedule specified on Blackboard.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:40-07:00"
    },
    {
      "path": "index.html",
      "title": "Statistical Machine Learning",
      "description": "American University 427/627\n",
      "author": [],
      "contents": "\nThis website contains most of the information and material that will be used for one of the sections of the course Statistical Machine Learning at American University.\nThe navigation bar contains information about the syllabus, schedule, readings, labs and assignments.\nLicense\nThis online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\nColophon\nThis book was written in RStudio using distill. The complete source is available on GitHub. All packages versions are being handled with renv.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:40-07:00"
    },
    {
      "path": "labs-02.html",
      "title": "Labs - Week 2",
      "author": [],
      "contents": "\nThese sets of labs will introduce you to linear models. Both simple and multiple. This will also be your first introduction to the parsnip package which we will use to specify models.\nExercise 1 - Simple linear regression\nLoad the data biomass and plot HHV as a function of carbon.\n\n\nlibrary(tidymodels)\ndata(\"biomass\")\n\n\n\nFit a simple linear regression model to the data. With HHV as the response and carbon as the predictor. Is it a good fit?\nUse the model to predict what the HHV of samples with carbon = 10, 20, ..., 80.\nPlot the fitted line of the linear model.\nProduce diagnostics plots. You can use plot() on the $fit object to produce some diagnostics.\nExercise 2 - Multiple linear regression\nFit a linear regression model to the data. With HHV as the response and carbon and hydrogen as the predictor. How is the fit compared to the simple linear model?\nFit a linear regression model to the data. With HHV as the response and all the molecules as the predictor. How is the fit compared to the previous models?\n\n\n\n",
      "last_modified": "2021-03-21T01:41:41-07:00"
    },
    {
      "path": "labs-03.html",
      "title": "Labs - Week 3",
      "author": [],
      "contents": "\nThese sets of labs will introduce you to logistic regression. This will also be your first introduction to the rsample package which we will use to perform train-test split.\nExercise 1\nIn this exercise we will explore the mlc_churn data set included in tidymodels.\n\n\nlibrary(tidymodels)\ndata(\"mlc_churn\")\n\n\n\nThe data set contains a variable called churn\nCreate a test-train rsplit object of mlc_churn using initial_split(). Use the arguments to set the proportions of the training data to be 80%. Stratify the sampling according to the churn variable. How many observations are in the testing and training sets?\nCreate the training and testing data set with training() and testing() respectively. Does the observation counts match what you found in the last question?\nFit a logistic regression model using logistic_reg(). Use number_vmail_messages, total_intl_minutes, total_intl_calls, total_intl_charge, number_customer_service_calls as predictors. Remember to fit the model only using the training data set.\nInspect the model with summary() and tidy(). How good are the variables we have chosen?\nPredict values for the testing data set. Use the type argument to also get probability predictions.\nUse conf_mat() to construct a confusion matrix. Does the confusion matrix look good?\nconf_mat() is used as follows, where truth is the name of the true response variable and estimate is the name of the predicted response.\n\n\ndata %>%\n  conf_mat(truth, estimate)\n\n\n\n\n\n\n",
      "last_modified": "2021-03-21T01:41:42-07:00"
    },
    {
      "path": "labs-04.html",
      "title": "Labs - Week 4",
      "author": [],
      "contents": "\nWe will be using the add-on package discrim to access functions to perform discriminant analysis models with parsnip and the klaR package to perform the QDA calculations. if you haven’t already got it installed run\n\n\ninstall.packages(c(\"discrim\", \"klaR\"))\n\n\n\nCreate a test-train rsplit object of mlc_churn using initial_split(). Use the arguments to set the proportions of the training data to be 80%. Stratify the sampling according to the churn variable.\nDo the following tasks for LDA, QDA and KNN model.\nFit a classification model. Use number_vmail_messages, total_intl_minutes, total_intl_calls, total_intl_charge, number_customer_service_calls as predictors. Remember to fit the model only using the training data set.\nInspect the model with summary() and tidy(). How good are the variables we have chosen?\nPredict values for the testing data set.\nUse conf_mat() to construct a confusion matrix. Does the confusion matrix look good?\n\n\n\n",
      "last_modified": "2021-03-21T01:41:43-07:00"
    },
    {
      "path": "labs-05.html",
      "title": "Lab 5 - k-fold cross-validation",
      "author": [],
      "contents": "\nCreate a test-train rsplit object of mlc_churn using initial_split(). Use the arguments to set the proportions of the training data to be 80%. Stratify the sampling according to the churn variable.\nCreate a LDA model specification.\nCreate a 10-fold cross-validation split object.\nFit the model within each of the folds.\nExtract the performance metrics for each fold.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:44-07:00"
    },
    {
      "path": "labs-06.html",
      "title": "Lab 6 - Bootstrapping",
      "author": [],
      "contents": "\nCreate a test-train rsplit object of mlc_churn using initial_split(). Use the arguments to set the proportions of the training data to be 80%. Stratify the sampling according to the churn variable.\nCreate a LDA model specification.\nCreate 10 bootstrap samples split object.\nFit the model within each of the folds.\nExtract the performance metrics for each fold.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:44-07:00"
    },
    {
      "path": "labs-07.html",
      "title": "Lab 7 - clustering",
      "author": [],
      "contents": "\nWe will be using the palmerpenguins data set for this lab.\nYou will also be needing to load the broom package\n\n\nlibrary(palmerpenguins)\nlibrary(broom)\n\n\n\nTransform the data set into a matrix using two of the numeric variables\nPerform k-means using 3 clusters\nLook at the result object with summary(), names(), and str().\nUse augment(), glance() and tidy() to extract information from the model\nPlot the clusters with your package of choice\nRerun The previous steps with more variables and different values of \\(K\\)\nConstruct an Elbow Chart to find an appropriate number of clusters for the data set\n\n\n\n",
      "last_modified": "2021-03-21T01:41:45-07:00"
    },
    {
      "path": "labs-09.html",
      "title": "Lab week 9",
      "author": [],
      "contents": "\nWe will in this lab explore how principal components are calculated and analyzed.\nWe will be using the concrete data set from the {modeldata} package which is loaded with {tidymodels}.\n\n\nlibrary(tidymodels)\ndata(concrete)\n\n\n\nCalculate the PCA of the data\nExplore the loading, eigenvalues, and final projection using the broom package\nVisualize the projections. Look at how the scaling of the variables changes the projection\nUse the {recipes} package to calculate the principal components.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:45-07:00"
    },
    {
      "path": "labs-10.html",
      "title": "Lab week 10",
      "author": [],
      "contents": "\nThis week we will talk about shrinkage and hyperparameter tuning.\nWe will use the Hitters data set from the ISLR library. It can be loaded using the following code\n\n\nlibrary(ISLR)\ndata(\"Hitters\")\n\n\n\nRemove all rows where the salary is NA and split the data into testing and training data sets.\nUse linear_reg() with mixture = 0 to specify a ridge regression model.\nFit the model on the data and inspect the model. What do you see?\nTry to predict using this model. What are your output?\nUse {tune} to setup hyperparameter tuning to determine the right amount of regularization.\nFit the best model. How does your parameter estimates look like?\n\n\n\n",
      "last_modified": "2021-03-21T01:41:46-07:00"
    },
    {
      "path": "labs.html",
      "title": "Labs",
      "author": [],
      "contents": "\nEach lecture will be followed by lab time which is sometimes where we will be using R and tidymodels to implement the methods we will have talked about that week.\nLabs are to be turned in on Blackboard. Albs will be available no later than 3 days before class and are due to be turned in on the following Sunday. We will be collectively be talking about the labs and come up with some of the answers. To get full credit for labs the document that is turned in should contain not just the right code to solve the problem but also text explaining what has been done.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:47-07:00"
    },
    {
      "path": "readings-02.html",
      "title": "Readings - week 2",
      "author": [],
      "contents": "\nRead chapter 3 of “An Introduction to Statistical Learning”. This chapter goes over simple linear regression, multiple linear regression, and the considerations used in linear regression. As far as I can tell, you have all taken courses where linear regression has been introduced. It is still worthwhile for you to read this chapter to get familiar with the book, and hopefully, it will be an easy read.\nIf you want more information on the theoretical background you can read sections 3.1 and 3.2 of “The Elements of Statistical Learning”. This is optional reading.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download\n\n\n\n",
      "last_modified": "2021-03-21T01:41:47-07:00"
    },
    {
      "path": "readings-03.html",
      "title": "Readings - week 3",
      "author": [],
      "contents": "\nStart read chapter 4 of “An Introduction to Statistical Learning”, we will color 4.1, 4.2 and 4.3. This chapter covers logistic regression, linear discriminant analysis, and quadratic discriminant analysis. We will start by talking about logistic regression this week and talk about linear discriminant analysis and quadratic discriminant analysis next week. Furthermore this is the first we will be talking about classification methods, take note of the differences between this and regression.\nWe will also talk about the notion of data splitting or “train-test splitting”. Please read the 5th chapter of “Tidy Modeling with R” which can be found here.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download\n\n\n\n",
      "last_modified": "2021-03-21T01:41:48-07:00"
    },
    {
      "path": "readings-04.html",
      "title": "Readings - week 4",
      "author": [],
      "contents": "\nThis week will be covering Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA) and K-Nearest Neighbors (KNN). Please finish reading chapter 4 (4.4 and 4.5) in “An Introduction to Statistical Learning”. If you haven’t already please make brush up on section 3.5 where KNN is covered for regression.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download\n\n\n\n",
      "last_modified": "2021-03-21T01:41:49-07:00"
    },
    {
      "path": "readings-05.html",
      "title": "Readings - week 5",
      "author": [],
      "contents": "\nPlease read chapter 5 in “An Introduction to Statistical Learning”, pay special attention to cross-validation which will be the focus on this week. We will also be talking about evaluating model performance so please also read chapter 9 - Judging model effectiveness in \"Tidy Modeling with R, which we will use as the introduction for the R package yardstick.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download\n\n\n\n",
      "last_modified": "2021-03-21T01:41:49-07:00"
    },
    {
      "path": "readings-06.html",
      "title": "Readings - week 6",
      "author": [],
      "contents": "\nWe will continue working on chapter 5 from “An Introduction to Statistical Learning” this week with a focus on bootstrapping and other kinds of Cross Validation. We will also talk about data leakage some more.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download\n\n\n\n",
      "last_modified": "2021-03-21T01:41:50-07:00"
    },
    {
      "path": "readings-07.html",
      "title": "Readings - week 7",
      "author": [],
      "contents": "\nPlease read chapter 10 in “An Introduction to Statistical Learning”. This will be the only major jump on the textbook, but it shouldn’t affect reading too much. The chapter covers clustering methods and we will look at \"K-Means Clustering and Hierarchical clustering. This will (sadly) be our only peak at unsupervised learning this course. Also read the article K-means clustering with tidy data principles from tidymodels.org which covers how to work with clustering in the tidymodels framework.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download\n\n\n\n",
      "last_modified": "2021-03-21T01:41:50-07:00"
    },
    {
      "path": "readings-08.html",
      "title": "Readings - week 8",
      "author": [],
      "contents": "\nWe are having Wellness Week. I’ll be talking about feature engineering and the value of pre-processing. There will be no reading this week.\nIf you find the subject of this lecture, you should check out the book Feature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\n\n\n\n",
      "last_modified": "2021-03-21T01:41:51-07:00"
    },
    {
      "path": "readings-09.html",
      "title": "Readings - week 9",
      "author": [],
      "contents": "\nWe will continue the topic of pre-processing from last week and see how it related to PCA (chapter 10 in “An Introduction to Statistical Learning”) and PCA regression (chapter 6.2).\nrecipes will be introduces as the R package we will use to perform pre-processing. Please read this vvignette on introducing recipes. We will also talk about the workflows, no reading required.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download\n\n\n\n",
      "last_modified": "2021-03-21T01:41:52-07:00"
    },
    {
      "path": "readings-10.html",
      "title": "Readings - week 10",
      "author": [],
      "contents": "\nPlease finish reading chapter 6 of “An Introduction to Statistical Learning”. The chapter covers Shrinkage methods including ridge and lasso regression.\nThis will be a good time to finally talk about hyper parameter tuning. We will use the tune package and dials package. A great place to start learning about these packages is this post.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:52-07:00"
    },
    {
      "path": "readings-11.html",
      "title": "Readings - week 11",
      "author": [],
      "contents": "\nWe will be talking about splines and Generalized additive models (GAM). Please read chapter 7 in “An Introduction to Statistical Learning”.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:53-07:00"
    },
    {
      "path": "readings-12.html",
      "title": "Readings - week 12",
      "author": [],
      "contents": "\nWe will be talking about Decision trees, bagging, boosting and random forests . Please read chapter 8 in “An Introduction to Statistical Learning”.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:53-07:00"
    },
    {
      "path": "readings-13.html",
      "title": "Readings - week 13",
      "author": [],
      "contents": "\nWe will be talking about Support Vector Machines. Please read chapter 9 in “An Introduction to Statistical Learning”.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:54-07:00"
    },
    {
      "path": "readings.html",
      "title": "Readings",
      "author": [],
      "contents": "\nThe readings for this course will be based on the textbooks listed in the syllabus. Readings will primarily be taken from the main textbook, but will sometimes be other material. Additional optional reading will also be listed for each week for the students who want to dig deeper into the content of the week. Information from the optional reading will not be expected knowledge in the homework and the exams and should be seen as a learning opportunity only.\nReadings will be listed no later than 3 days before class. Reading before class is strongly encouraged to allow you to find the areas you have additional questions about and ask them in class.\nSlides will also be posted here for each week.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:54-07:00"
    },
    {
      "path": "reference.html",
      "title": "Reference",
      "author": [],
      "contents": "\nVarious information links will be posted here as needed.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:55-07:00"
    },
    {
      "path": "schedule.html",
      "title": "Schedule",
      "author": [],
      "contents": "\n\nContents\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\n\n\n\n\n\n\n\nWeek 1\n2021-01-17 to 2021-01-23\nTopics: Introductions, What is Statistical Machine Learning?, R, RStudio, Tidymodels\nreadings\nlabs\nNo Assignment this week\nWeek 2\n2021-01-24 to 2021-01-30\nTopics: Linear Regression, Regression\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 3\n2021-01-31 to 2021-02-06\nTopics: Logistic Regression, Classification, Train-Test Split\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 4\n2021-02-07 to 2021-02-13\nTopics: LDA, QDA, K-Nearest Neighbors\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 5\n2021-02-14 to 2021-02-20\nTopics: Bootstrap, Model Diagnostics, Evaluation Metrics\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 6\n2021-02-21 to 2021-02-27\nTopics: Cross Validation\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 7\n2021-02-28 to 2021-03-06\nTopics: Clustering, K-Means Clustering, Hierarchical Clustering\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nMidterm\nWeek 8\n2021-03-07 to 2021-03-13\nWellness Week\nTopics: Feature Engineering, Data Preprocessing\nNo readings, labs or assignment due to Wellness Week\nWeek 9\n2021-03-14 to 2021-03-20\nTopics: PCA, PCA Regression\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 10\n2021-03-21 to 2021-03-27\nTopics: Shrinkage Methods, Rigde, Lasso, Hyper Parameter Tuning\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 11\n2021-03-28 to 2021-04-03\nTopics: Splines, GAM\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 12\n2021-04-04 to 2021-04-10\nTopics: Decision Trees, Bagging, Boosting, Random Forrests\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 13\n2021-04-11 to 2021-04-17\nTopics: SVM\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 14\n2021-04-18 to 2021-04-24\nTopics: Final Project\nReadings haven’t been posted yet\nLabs haven’t been posted yet\nNo assignment this week. Time will be dedicated for Final Project\n\n\n\n",
      "last_modified": "2021-03-21T01:41:56-07:00"
    },
    {
      "path": "syllabus.html",
      "title": "Syllabus",
      "author": [],
      "contents": "\n\nContents\nInstructor\nPre-requisites\nTextbooks\nMain textbook\nSupplementary books\nArticles, book chapters, and other materials\n\nCourse Plan\nAssignments and grading\nLearning objectives\nOnline help\nSoftware\nLearning during a pandemic\nLauren’s Promise\nSupport Services\nEmergency preparedness\nMathematics & Statistics Tutoring Lab (Don Myers Building)\nAcademic Support and Access Center\nCenter for Diversity & Inclusion (X3651, MGC 201)\nThe Office of Advocacy Services for Interpersonal and Sexual Violence (X7070)\nCounseling Center (x3500)\nReligious Holidays\nAcademic Integrity Code\n\n\nInstructor\nInstructor: Emil Hvitfeldt\nTime: Tuesday 5:30PM - 8:00PM ET time zone (Washington DC time)\nCourse website: https://emilhvitfeldt.github.io/AU-2021spring-627/index.html\nOffice hours: Sunday 6:00PM - 7:00PM\nEmail: emilh@american.edu\nTwiter: @Emil_Hvitfeldt\nE-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 24 hours (really) but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry!\nPre-requisites\nSTAT 520 “Applied Multivariate Analysis” or STAT 615 “Regression”.\nTextbooks\nMain textbook\nThis book will be required reading and we will aim to cover most of the content.\n“An Introduction to Statistical Learning with Applications in R” by G. James, D. Witten, T. Hastie, and R. Tibshirani; Springer, 2013. ISBN 1461471370. The latest corrected printing is available on James’s page at https://statlearning.com/\nSupplementary books\nThese books are by no means necessary to buy or read to complete this course but serve as great stepping stones for deeper study. Some week’s readings will refer to these books for extra readings.\n“The Elements of Statistical Learning: Data Mining, Inference, and Prediction”, by T. Hastie, R. Tibshirani, and J. Friedman, 2nd Edition; Springer, 2009. ISBN 0387848576. Available on Hastie’s page at https://web.stanford.edu/~hastie/Papers/ESLII.pdf [more technical; contains advanced explanations and mathematical proofs].\n“Tidy Modeling with R” by Max Kuhn and Julia Silge. Available online at https://www.tmwr.org/.\nArticles, book chapters, and other materials\nThere will also occasionally be additional articles and videos to read and watch. When this happens, links to these other resources will be included on the reading page for that week.\nCourse Plan\nIntroduction, motivation, and examples. Understanding large and complex data sets. Statistical learning. First steps in R. [Chap. 1-2].\nReview of regression modeling and analysis; implementation in R. [Chap. 3].\nClassification problems and classification tools. Logistic regression and review of linear discriminant analysis. [Chap. 4]\nResampling methods; bootstrap. [Chap. 5 and lecture notes].\nHigh-dimensional data and shrinkage. Ridge regression. LASSO. Model selection methods and dimension reduction. Principal components. Partial least squares. [Chap. 6]\nNonlinear trends and splines. [Chap. 7; 7.4-7.5]\nRegression trees and decision trees [Chap. 8]\nIntroduction to support vector machines [Chap. 9]\nClustering methods [Chap. 10]\nAdditional topics and applications, if time permits.\nAssignments and grading\nAssignments (35%): During the semester I will assign, collect, and grade assignments. You may receive assistance from other students in the class and me, but your submissions must be composed of your own thoughts, coding, and words. A typical homework will include a few problems to do by hand, to see how things work, and a few realistic problems to do using R. Late submission is accepted at a cost of a 5% deduction for each day, with a maximum deduction of 50%.\nLabs (20%): 30-45 minute labs at the end of each class. Each lab covers the material of the lecture. You will have to submit the solutions of each lab on Blackboard the Sunday after each class.\nMidterm (15%): The midterm covers several chapters of the material and it is take home. You will have three days to complete the exam.\nProject (25%) (20% report 5% presentation): Each student will receive or choose a data set with data description, problem formulation, and instructions. Using sound statistical methods, you will do the necessary modeling and data analysis and write a report summarizing your results and answering specific questions of your project. A 10-minute presentation summarizing the report will be given to the class or submitted on Blackboard.\n90 – 100 % = A\n87 – 90 % = A-\n83 – 87 % = B+\n80 – 83 % = B\n77 – 80 % = B-\n73 – 77 = C+\n70 – 73 % = C\n60 – 70 % = C-\nPlease schedule a meeting with me if you would like to see or discuss your grade at any point during the semester.\nLearning objectives\nGraduate students (STAT 627)\nStudents will be able to:\nIdentify appropriate statistical learning methods for the given problem involving real data.\nUnderstand the underlying assumptions, verify them, and propose appropriate actions if some assumptions do not hold.\nIdentify other possible problems with messy data, such as multicollinearity, understand their consequences, and propose solutions.\nEvaluate the performance of the chosen regression and classification techniques and compare them.\nApply cross-validation techniques to find the optimal degree of flexibility - the best subset of predictors or the optimal tuning parameters.\nShow, analytically, or empirically, the optimal balance between precision within training data and prediction power.\nIllustrate results with appropriate plots and diagrams.\nUndergraduate students (STAT 427)\nStudents will be able to:\nIdentify appropriate statistical learning methods for the given problem involving real data.\nUnderstand the underlying assumptions, techniques available to verify them, and propose appropriate remedies.\nUse training and testing data to evaluate the performance of the chosen regression and classification techniques and compare them.\nUse available empirical tools to find the optimal balance between precision within training data and prediction power.\nIllustrate results with appropriate plots and diagrams.\nStudents will demonstrate competence in using different statistical learning methods involving large, messy, and multi-dimensional numerical and categorical data. Methods include linear, logistic, and polynomial regression with proper variable selection, linear and quadratic discriminant analysis, K-nearest neighbor classifier, bootstrap, ridge regression, lasso, principal components regression, partial least squares, splines, regression and classification trees, support vector machines, clustering, and related methods. In addition, graduate students (STAT 627) will demonstrate competency in the analytic justification of the chosen methods, tuning of the algorithms, and evaluating their prediction power.\nOnline help\nData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately, there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Twitter, post R-related questions, and content with #rstats. The community there is exceptionally generous and helpful.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot,” but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”).\nAdditionally, we have a class chatroom at Slack where anyone in the class can ask questions and anyone can answer. I will monitor Slack regularly and will respond quickly. Ask questions about the readings, assignments, and project. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other people’s questions too.\nSoftware\nWe will be using R and tidymodels in this class. While not required, it is highly recommended that you use an IDE for R, I recommend https://rstudio.com/products/rstudio/.\nLearning during a pandemic\nLife absolutely sucks right now. None of us is really okay. We’re all just pretending.\nYou most likely know people who have lost their jobs, have tested positive for COVID-19, have been hospitalized, or perhaps have even died. You all have increased (or possibly decreased) work responsibilities and increased family care responsibilities—you might be caring for extra people (young and/or old!) right now, and you are likely facing uncertain job prospects (or have been laid off!).\nI’m fully committed to making sure that you learn everything you were hoping to learn from this class! I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Under pandemic conditions, that flexibility and leniency are intensified.\nIf you tell me you’re having trouble, I will not judge you or think less of you. I hope you’ll extend me the same grace.\nYou never owe me personal information about your health (mental or physical). You are always welcome to talk to me about things that you’re going through, though. If I can’t help you, I usually know somebody who can.\nIf you need extra help, or if you need more time with something, or if you feel like you’re behind or not understanding everything, do not suffer in silence! Talk to me! I will work with you. I promise.\nLauren’s Promise\nI will listen and believe you if someone is threatening you.\nLauren McCluskey, a 21-year-old honors student-athlete, was murdered on October 22, 2018, by a man she briefly dated on the University of Utah campus. We must all take action to ensure that this never happens again.\nIf you are in immediate danger, call 911 or AU police (202 885-2527).\nIf you are experiencing sexual assault, domestic violence, or stalking, please report it to me and I will connect you to resources or find appropriate contact information for Counseling Center.\nSupport Services\nEmergency preparedness\nIn the event of an emergency, students should refer to the AU Web site http: //www.american.edu/emergency and the AU information line at (202) 885-1100 for general university-wide information. In case of a prolonged closure of the University, I send updates to you by email and will post all announcements on Blackboard.\nMathematics & Statistics Tutoring Lab (Don Myers Building)\nprovides tutoring in Intermediate Mathematics and Statistics. http://www.american.edu/cas/mathstat/tutoring.cfm\nAcademic Support and Access Center\noffers study skills workshops, individual instruction, tutor referrals, Supplemental Instruction, writing support, and technical and practical support and assistance with accommodations for students with physical, medical, or psychological disabilities. Writing support is also available in the Writing Center, Battelle-Tompkins 228.\nCenter for Diversity & Inclusion (X3651, MGC 201)\nis dedicated to enhancing LGBTQ, Multicultural, First Generation, and Women’s experiences on campus and to advance AU’s commitment to respecting & valuing diversity by serving as a resource and liaison to students, staff, and faculty on issues of equity through education, outreach, and advocacy.\nThe Office of Advocacy Services for Interpersonal and Sexual Violence (X7070)\nprovides free and confidential advocacy services for anyone in the campus community who is impacted by sexual violence (sexual assault, dating or domestic violence, and stalking).\nCounseling Center (x3500)\noffers counseling and consultations regarding personal concerns, self-help information, and connections to off-campus mental health resources. Academic Support and Access Center (x3360) offers study skills workshops, individual instruction, tutor referrals, Supplemental Instruction, writing support, and technical and practical support and assistance with accommodations for students with physical, medical, or psychological disabilities.\nReligious Holidays\nStudents may receive accommodation in the course for the observance of a religious and/or cultural holiday. The student should notify the professor as soon as possible should such a need exist. More information about accommodations for religious and/or cultural holidays can be found at www.american.edu/ocl/kay/request-for-religious-accommodation.cfm.\nAcademic Integrity Code\nPlease be sure that you are familiar with AU’s Academic Integrity Code, as I am required to report any cases of academic dishonesty to the dean of CAS. For your review: http://www.american.edu/academics/ integrity/.\n\n\n\n",
      "last_modified": "2021-03-21T01:41:57-07:00"
    }
  ],
  "collections": []
}
